{"cells":[{"cell_type":"code","execution_count":null,"id":"1fca6c48-5c7b-4a06-87ae-54a3a3f8b272","metadata":{"id":"1fca6c48-5c7b-4a06-87ae-54a3a3f8b272"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import load_model\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","from sklearn.metrics import (\n","    roc_curve, auc,\n","    precision_recall_curve, average_precision_score,\n","    confusion_matrix, f1_score, roc_auc_score\n",")\n","import glob"]},{"cell_type":"markdown","id":"93ef4e6c-44b2-4498-8c3f-c5eba55e0a13","metadata":{"id":"93ef4e6c-44b2-4498-8c3f-c5eba55e0a13"},"source":["File Creation"]},{"cell_type":"code","execution_count":null,"id":"1382abe5-9089-4c0d-a298-e272706ed634","metadata":{"id":"1382abe5-9089-4c0d-a298-e272706ed634"},"outputs":[],"source":["os.makedirs(\"ep_robust_viz\", exist_ok=True)\n","os.makedirs(\"ep_robust_log\", exist_ok=True)"]},{"cell_type":"markdown","id":"6d1ac7f8-b193-4ae9-bd5b-eec52f6ef1a0","metadata":{"id":"6d1ac7f8-b193-4ae9-bd5b-eec52f6ef1a0"},"source":["Plotting Functions"]},{"cell_type":"code","execution_count":null,"id":"8d1a699d-66e6-4434-9520-53bfd1bfeab7","metadata":{"id":"8d1a699d-66e6-4434-9520-53bfd1bfeab7"},"outputs":[],"source":["def plot_avg_history(epochs, avg_metrics, std_metrics, epsilons):\n","    plt.figure(figsize=(12, 6))\n","\n","    # Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, avg_metrics['loss'], label='Train Loss')\n","    plt.fill_between(epochs,\n","                     np.array(avg_metrics['loss']) - np.array(std_metrics['loss']),\n","                     np.array(avg_metrics['loss']) + np.array(std_metrics['loss']),\n","                     alpha=0.2)\n","    plt.plot(epochs, avg_metrics['val_loss'], label='Val Loss')\n","    plt.fill_between(epochs,\n","                     np.array(avg_metrics['val_loss']) - np.array(std_metrics['val_loss']),\n","                     np.array(avg_metrics['val_loss']) + np.array(std_metrics['val_loss']),\n","                     alpha=0.2)\n","    plt.title(\"Loss over Epochs\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","\n","    # Accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, avg_metrics['accuracy'], label='Train Accuracy')\n","    plt.fill_between(epochs,\n","                     np.array(avg_metrics['accuracy']) - np.array(std_metrics['accuracy']),\n","                     np.array(avg_metrics['accuracy']) + np.array(std_metrics['accuracy']),\n","                     alpha=0.2)\n","    plt.plot(epochs, avg_metrics['val_accuracy'], label='Val Accuracy')\n","    plt.fill_between(epochs,\n","                     np.array(avg_metrics['val_accuracy']) - np.array(std_metrics['val_accuracy']),\n","                     np.array(avg_metrics['val_accuracy']) + np.array(std_metrics['val_accuracy']),\n","                     alpha=0.2)\n","    plt.title(\"Accuracy over Epochs\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"ep_robust_viz/{epsilons[idx]}/history_curves.png\")\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"id":"04193c68-dc0c-4f7b-971f-4e90356c7a5e","metadata":{"id":"04193c68-dc0c-4f7b-971f-4e90356c7a5e"},"outputs":[],"source":["def plot_mean_se_with_baseline(mean_se_values, x_values, x_label, y_label, filename,\n","                                baseline, baseline_label=\"Baseline\"):\n","\n","    os.makedirs(\"robust_viz\", exist_ok=True)\n","\n","    means = np.array([m for m, se in mean_se_values])\n","    ses = np.array([se for m, se in mean_se_values])\n","\n","    # Extend data with baseline\n","    baseline_mean, baseline_se = baseline\n","    means = np.append(means, baseline_mean)\n","    ses = np.append(ses, baseline_se)\n","    x_all = list(x_values) + [baseline_label]\n","\n","    x_ticks = np.arange(len(x_all))\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.errorbar(x_ticks, means, yerr=ses, fmt='o-', capsize=5,\n","                 color='steelblue', ecolor='gray', elinewidth=2, marker='o')\n","\n","    plt.xticks(ticks=x_ticks, labels=x_all, rotation=45)\n","    plt.xlabel(x_label)\n","    plt.ylabel(y_label)\n","    plt.title(f\"{y_label} across epsilons with SE and Baseline\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig(f\"robust_viz/{filename}\")\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"id":"1602425f-6a52-406c-b6e9-de7d77a7c38c","metadata":{"id":"1602425f-6a52-406c-b6e9-de7d77a7c38c"},"outputs":[],"source":["def plot_confusion_matrix_with_se(conf_matrix, annotations, epsilons):\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(conf_matrix, annot=annotations, fmt='', cmap=\"Blues\", cbar=False, square=True,\n","                xticklabels=['Predicted 0', 'Predicted 1'],\n","                yticklabels=['Actual 0', 'Actual 1'])\n","\n","    plt.title(f'Average Confusion Matrix with SE for epsilon {{epsilons[idx]}}')\n","    plt.xlabel('Prediction')\n","    plt.ylabel('Actual')\n","    plt.tight_layout()\n","    plt.savefig(f\"ep_robust_viz/{epsilons[idx]}/avg_confusion_matrix.png\")\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"id":"54d57582-790e-4e44-acc9-60cc1e8beb45","metadata":{"id":"54d57582-790e-4e44-acc9-60cc1e8beb45"},"outputs":[],"source":["def plot_metric_distribution(values, metric_name, filename, epsilons):\n","    mean, ci_lower, ci_upper = np_95ci(values)\n","\n","    plt.figure(figsize=(8, 5))\n","    plt.hist(values, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n","    plt.axvline(mean, color='red', linestyle='--', label=f'Mean = {mean:.3f}')\n","    plt.axvline(ci_lower, color='green', linestyle=':', label=f'95% CI Lower = {ci_lower:.3f}')\n","    plt.axvline(ci_upper, color='green', linestyle=':', label=f'95% CI Upper = {ci_upper:.3f}')\n","\n","    plt.title(f'{metric_name} Distribution with 95% CI')\n","    plt.xlabel(metric_name)\n","    plt.ylabel('Frequency')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig(f\"ep_robust_viz/{epsilons[idx]}/{filename}\")\n","    plt.close()"]},{"cell_type":"markdown","id":"de51e955-0ddb-42c9-88a8-e42f98facf9e","metadata":{"id":"de51e955-0ddb-42c9-88a8-e42f98facf9e"},"source":["STATS FUNCTIONS"]},{"cell_type":"code","execution_count":null,"id":"70d31bab-642c-4018-9e60-f2ee68aec1fd","metadata":{"id":"70d31bab-642c-4018-9e60-f2ee68aec1fd"},"outputs":[],"source":["def np_95ci(data):\n","    mean = np.mean(data)\n","    std = np.std(data, ddof=1)  # sample standard deviation\n","    se = std / np.sqrt(len(data))\n","    ci_lower = mean - 1.96 * se\n","    ci_upper = mean + 1.96 * se\n","    return mean, ci_lower, ci_upper"]},{"cell_type":"code","execution_count":null,"id":"86bd258b-4e13-45d0-a5e7-579e0d0c994a","metadata":{"id":"86bd258b-4e13-45d0-a5e7-579e0d0c994a"},"outputs":[],"source":["def mean_se(values):\n","    values = np.array(values)\n","    return np.mean(values), np.std(values, ddof=1) / np.sqrt(len(values))"]},{"cell_type":"code","execution_count":null,"id":"d4a2ac54-b77e-4d55-b7c8-1d4752b1b560","metadata":{"id":"d4a2ac54-b77e-4d55-b7c8-1d4752b1b560"},"outputs":[],"source":["def bootstrap(x_train, y_train):\n","    x_train = pd.DataFrame(x_train)\n","    y_train = pd.DataFrame(y_train)\n","    k = len(x_train)\n","    idx = np.random.choice(k, size = k,  replace = True)\n","    return x_train.iloc[idx], y_train.iloc[idx]"]},{"cell_type":"markdown","id":"487c6efd-dbb9-4180-a628-49f6899f4d95","metadata":{"id":"487c6efd-dbb9-4180-a628-49f6899f4d95"},"source":["Preprocessing Functions"]},{"cell_type":"code","execution_count":null,"id":"ec75d7a7-293d-4844-ae0d-eb530d427b54","metadata":{"id":"ec75d7a7-293d-4844-ae0d-eb530d427b54"},"outputs":[],"source":["def noisydata(data: pd.DataFrame, epsilon: float) -> pd.DataFrame:\n","    \"\"\"\n","    Applies the Laplace mechanism to add noise to each feature column for differential privacy.\n","    Uses two-step mean + residual perturbation under a public bound of [0, 20], which covers the\n","    full range of log-transformed RNA-sequencing expression values in almost every practical dataset.\n","\n","    Parameters:\n","            data (pd.DataFrame): input dataset.\n","            epsilon (float): privacy budget for differential privacy.\n","\n","    Returns:\n","            pd.DataFrame: A DataFrame of the same shape as `data`, with Laplace noise added to each feature.\n","            The 'disease' column, if present, is left unchanged.\n","    \"\"\"\n","    # Check if the target variable 'disease' is present in the data.\n","    if 'disease' in data.columns:\n","        X = data.drop(columns = ['disease'])\n","    else:\n","        X = data.copy()\n","\n","    # Use public bounds [0, 20] for the features.\n","    a_public = pd.Series(0, index = X.columns)\n","    b_public = pd.Series(20, index = X.columns)\n","\n","    # Clip the data to public [a, b].\n","    X_clipped = X.clip(lower = a_public, upper = b_public, axis = 1)\n","\n","    # Split the privacy budget equally between the mean and residual perturbation.\n","    eps_mean = epsilon / 2\n","    eps_residual = epsilon / 2\n","\n","    # Compute the true column means.\n","    n = len(X_clipped)\n","    mu = X_clipped.mean(axis = 0)\n","\n","    # Privatize the means with Laplace ((U-L)/(n*eps_mean))\n","    sensitivity_mean = (b_public - a_public) / n\n","    scale_mean = sensitivity_mean / eps_mean\n","    noise_mean = np.random.laplace(loc = 0, scale = scale_mean.values, size = scale_mean.shape)\n","    mu_noisy = pd.Series(mu.values + noise_mean, index = X.columns)\n","\n","    # Center on the noisy means.\n","    R = X_clipped.subtract(mu_noisy, axis = 1)\n","\n","    # Compute the public residual bounds ((U-L)/2) = 10.\n","    D = (b_public - a_public) / 2\n","\n","    # Clip the residuals to public bounds.\n","    R_clipped = R.clip(lower = -D, upper = D, axis = 1)\n","\n","    # Privatize residuals with Laplace (2D / eps_residual).\n","    sensitivity_residual = 2 * D\n","    scale_residual = sensitivity_residual / eps_residual\n","    noise_residual = np.random.laplace(loc = 0 , scale = scale_residual.values, size = R_clipped.shape)\n","    R_noisy = pd.DataFrame(R_clipped.values + noise_residual, columns = X.columns, index = X.index)\n","    X_noisy = R_noisy.add(mu_noisy, axis = 1)\n","    noisydata = X_noisy\n","\n","    # If 'disease' column is present, add it back to the noisy data.\n","    if 'disease' in data.columns:\n","        noisydata['disease'] = data['disease']\n","\n","    # Return the noisy data.\n","    return noisydata"]},{"cell_type":"code","execution_count":null,"id":"08d85cbc-b94f-412b-8bbc-560e7b8214ba","metadata":{"id":"08d85cbc-b94f-412b-8bbc-560e7b8214ba"},"outputs":[],"source":["def data_gen(epsilons, test_size =.2, random_state = 42):\n","    data_holder = []\n","    for idx, ep in enumerate(epsilons):\n","        noisy_data = noisydata(data, ep)\n","\n","        X = noisy_data.drop(columns=['disease']).values\n","        y = noisy_data['disease'].values\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n","        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)\n","        scaler = StandardScaler()\n","        X_train = scaler.fit_transform(X_train)\n","        X_test = scaler.transform(X_test)\n","        X_val = scaler.transform(X_val)\n","\n","        data_holder.append((X_train, X_test, y_train, y_test, X_val, y_val))\n","\n","    return data_holder"]},{"cell_type":"markdown","id":"acbe4b79-8188-4d74-8a13-c6446bb804df","metadata":{"id":"acbe4b79-8188-4d74-8a13-c6446bb804df"},"source":["Preprocessing"]},{"cell_type":"code","execution_count":null,"id":"387ef135-f890-4be4-b2fa-e12c4c223ffc","metadata":{"id":"387ef135-f890-4be4-b2fa-e12c4c223ffc"},"outputs":[],"source":["#TODO: You may need to fix file paths to whatever this is on your cluster\n","data = pd.read_csv('../data/endometriosis_dataset.csv')\n","epsilons = [.001, .01, .1, 1, 10]"]},{"cell_type":"code","execution_count":null,"id":"a15b13be-7016-4150-a1f9-f594b2dc099f","metadata":{"id":"a15b13be-7016-4150-a1f9-f594b2dc099f"},"outputs":[],"source":["dataset = data_gen(epsilons, test_size =.2, random_state = 42)"]},{"cell_type":"markdown","id":"969ffae7-273d-47a3-a975-77e61ed1022a","metadata":{"id":"969ffae7-273d-47a3-a975-77e61ed1022a"},"source":["Model Training Functions"]},{"cell_type":"code","execution_count":null,"id":"017d23f2-2e55-46f4-b74d-7de42a1a4046","metadata":{"id":"017d23f2-2e55-46f4-b74d-7de42a1a4046"},"outputs":[],"source":["def model_train(data):\n","\n","    for idx in range(len(data)):\n","\n","        for i in range(50):\n","\n","            X_boot, y_boot = bootstrap(data[idx][0])\n","\n","\n","            DP_model = Sequential([\n","            Dense(512, activation='relu', input_shape=(data[idx][0].shape[1],)),\n","            Dropout(0.5),\n","            Dense(128, activation='relu'),\n","            Dropout(0.5),\n","            Dense(1, activation='sigmoid')\n","        ])\n","            DP_model.compile(optimizer=Adam(learning_rate=1e-3),\n","                      loss='binary_crossentropy',\n","                      metrics=['accuracy'])\n","\n","            csv_logger = tf.keras.callbacks.CSVLogger(f\"ep_robust_log/{epsilons[idx]}/training_log_dp_{i}.csv\", append=True)\n","\n","            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=f\"models_dp_robust/{epsilons[idx]}/model_dp_{i}_{{epoch:02d}}.keras\",\n","            save_weights_only=False,\n","            save_best_only=False,  # Save every epoch\n","            verbose=1\n","            )\n","\n","            history_dp = DP_model.fit(\n","            X_boot,\n","            y_boot,\n","            epochs=50,\n","            batch_size=32,\n","            validation_data = (data[idx][4], data[idx][5]),\n","            callbacks = [csv_logger, checkpoint]\n","            )"]},{"cell_type":"markdown","id":"60db13ca-5fbd-4bcc-9300-f629b33ec999","metadata":{"id":"60db13ca-5fbd-4bcc-9300-f629b33ec999"},"source":["Model Training"]},{"cell_type":"code","execution_count":null,"id":"7acb76a4-e3c5-441d-8bb8-3440ca8d0275","metadata":{"id":"7acb76a4-e3c5-441d-8bb8-3440ca8d0275"},"outputs":[],"source":["model_train(dataset)"]},{"cell_type":"markdown","id":"d693a87a-9500-4210-a731-9553dfb75248","metadata":{"id":"d693a87a-9500-4210-a731-9553dfb75248"},"source":["STATS Scripts"]},{"cell_type":"code","execution_count":null,"id":"43831baf-7a18-405b-9f8d-3f4172ca62d2","metadata":{"id":"43831baf-7a18-405b-9f8d-3f4172ca62d2"},"outputs":[],"source":["f1_total = []\n","auc_total = []\n","\n","for idx in range(len(epsilons)):\n","    # Step 1: Load all CSVs into a list of DataFrames\n","    csv_files = glob.glob(f\"ep_robust_log/{epsilons[idx]}/training_log_dp_*.csv\")\n","    histories = [pd.read_csv(f) for f in csv_files]\n","\n","    # Step 2: Stack the metrics for each epoch\n","    metrics = ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n","    avg_metrics = {m: [] for m in metrics}\n","    std_metrics = {m: [] for m in metrics}\n","    epochs = histories[0]['epoch']  # Assuming all runs have the same epoch range\n","\n","    for epoch in epochs:\n","        for metric in metrics:\n","            values = [h.loc[epoch, metric] for h in histories]\n","            avg_metrics[metric].append(np.mean(values))\n","            std_metrics[metric].append(np.std(values))\n","\n","\n","    model_paths = glob.glob(f\"models_dp_robust/{epsilons[idx]}/model_dp_*_50.keras\")\n","    model_paths.sort()\n","\n","    tp_list = []\n","    fp_list = []\n","    tn_list = []\n","    fn_list = []\n","    f1_scores = []\n","    auc_scores = []\n","\n","\n","    for model_path in model_paths:\n","        model = tf.keras.models.load_model(model_path)\n","        y_pred_prob = model.predict(X_test)\n","        y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","        f1 = f1_score(y_test, y_pred)\n","        auc = roc_auc_score(y_test, y_pred_prob)\n","\n","        f1_scores.append(f1)\n","        auc_scores.append(auc)\n","\n","        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","\n","        tp_list.append(tp)\n","        fp_list.append(fp)\n","        tn_list.append(tn)\n","        fn_list.append(fn)\n","\n","    # Compute mean and SE for each confusion matrix component\n","    tp_mean, tp_se = mean_se(tp_list)\n","    fp_mean, fp_se = mean_se(fp_list)\n","    tn_mean, tn_se = mean_se(tn_list)\n","    fn_mean, fn_se = mean_se(fn_list)\n","\n","    # Construct the matrix and annotation array\n","    conf_matrix = np.array([[tn_mean, fp_mean],\n","                            [fn_mean, tp_mean]])\n","\n","    annotations = np.array([[f\"{tn_mean:.1f}\\n±{tn_se:.1f}\", f\"{fp_mean:.1f}\\n±{fp_se:.1f}\"],\n","                            [f\"{fn_mean:.1f}\\n±{fn_se:.1f}\", f\"{tp_mean:.1f}\\n±{tp_se:.1f}\"]])\n","\n","    f1_total.append(mean_se(f1_scores))\n","    auc_total.append(mean_se(auc_scores))\n","\n","    plot_metric_distribution(f1_scores, \"F1 Score\", \"f1_score_robust\", epsilons)\n","    plot_metric_distribution(auc_scores, \"AUC Score\", \"AUC_score_robust\", epsilons)\n","    plot_confusion_matrix_with_se(conf_matrix, annotations, epsilons)\n","    plot_avg_history(epochs, avg_metrics, std_metrics, epsilons)"]},{"cell_type":"code","execution_count":null,"id":"fa69f234-a206-47da-a459-fa38fc0737fb","metadata":{"id":"fa69f234-a206-47da-a459-fa38fc0737fb"},"outputs":[],"source":["f1_baseline = (0.9659922307568748, 0.002160392362343558)\n","auc_baseline = (0.896936507936508, 0.00458851871947309)"]},{"cell_type":"code","execution_count":null,"id":"6203481c-af8c-405f-bd95-027120543ce3","metadata":{"id":"6203481c-af8c-405f-bd95-027120543ce3"},"outputs":[],"source":["plot_mean_se_with_baseline(f1_total, epsilons, \"epsilons\", \"Average F1 Score\", \"avg_f1\",\n","                                f1_baseline, baseline_label=\"baseline\")"]},{"cell_type":"code","execution_count":null,"id":"f501cbde-0cd7-4330-a401-602af1d7d32f","metadata":{"id":"f501cbde-0cd7-4330-a401-602af1d7d32f"},"outputs":[],"source":["plot_mean_se_with_baseline(auc_total, epsilons, \"epsilons\", \"Average AUC Score\", \"avg_auc\",\n","                                auc_baseline, baseline_label=\"baseline\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}